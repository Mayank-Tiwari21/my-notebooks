{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c15cd1e-330a-4cca-8d16-035af3d77eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x74c3b4deab30>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 16:06:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()  # Initialize findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark in Jupyter\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify SparkSession is created\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59dc29ae-babf-47bb-b002-d99151072bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#creating a dataframe from a csv file , infering schema and providing the header\n",
    "df = spark.read.csv('/home/hdoop/Docs/spark_files/file_CSV_10Mar2025_122000.csv',header = True,inferSchema =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d08f37b-417c-4941-ac37-264509604263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5a40caa-a4d6-4bbb-b6bf-d37134f5032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7884b4-da49-4d60-9e7c-f08511fd57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =spark.read.format(\"csv\").options(header = True,inferSchema =True).load('/home/hdoop/Docs/spark_files/file_CSV_10Mar2025_122000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d94a256-752d-4fcb-874a-bf6fc8b8c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e42b20ce-7642-47fe-8333-adda739153d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd5e1311-9999-4411-8833-2e8bd7f98122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.csv(path=['/home/hdoop/Docs/spark_files/group_loader_files/file1.csv','/home/hdoop/Docs/spark_files/group_loader_files/file2.csv'],inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5b29872-cdb2-43bc-89df-ad6ee81610e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+-----+\n",
      "|_c0|     _c1|    _c2|  _c3|\n",
      "+---+--------+-------+-----+\n",
      "|  4|himanshu|     HR|48000|\n",
      "|  5|   rohan|finance|78000|\n",
      "|  6|   arnav|finance|74000|\n",
      "|  1|  mayank|     IT|65000|\n",
      "|  2|     jay|     HR|45000|\n",
      "|  3|   Rinku|     IT|65000|\n",
      "+---+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "024e9b92-a49e-4764-8cdd-7b87ce98b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cf3a2b4-0ecb-4ded-afbd-b0e3a291ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.csv(path=['/home/hdoop/Docs/spark_files/group_loader_files/'],inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59eb89e8-7708-4947-b3a3-8b1021a89eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+-----+\n",
      "|_c0|     _c1|    _c2|  _c3|\n",
      "+---+--------+-------+-----+\n",
      "|  4|himanshu|     HR|48000|\n",
      "|  5|   rohan|finance|78000|\n",
      "|  6|   arnav|finance|74000|\n",
      "|  1|  mayank|     IT|65000|\n",
      "|  2|     jay|     HR|45000|\n",
      "|  3|   Rinku|     IT|65000|\n",
      "+---+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "702b0445-1df5-45f9-a354-340cf9129aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c501f090-3c6c-48e9-80df-f3d6f85bd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType()\\\n",
    "        .add(\"id\",IntegerType(),True)\\\n",
    "        .add(\"name\",StringType(),True)\\\n",
    "        .add(\"dept\",StringType(),True)\\\n",
    "        .add(\"salary\",IntegerType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e88076b3-501e-4d37-bf03-c0acc6e427c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.read.csv('/home/hdoop/Docs/spark_files/group_loader_files/',schema = schema,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "268a5507-710b-4f5b-a8d9-67c36ad04c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  4|himanshu|     HR| 48000|\n",
      "|  5|   rohan|finance| 78000|\n",
      "|  6|   arnav|finance| 74000|\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|     jay|     HR| 45000|\n",
      "|  3|   Rinku|     IT| 65000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eceb1ac-a999-4d63-b5a8-587fbe327bc5",
   "metadata": {},
   "source": [
    "### Write to a csv or parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2b28a62-bef0-41cc-92d2-2a6cf7005ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d3e8501-db5e-4ca9-ae05-ff747f7304f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(DataFrameWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f92e7924-ea25-4b05-b401-6583b6afb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26d434f4-4831-493a-aaf2-503542d817ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on property:\n",
      "\n",
      "    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      "    storage.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrameWriter`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "    >>> type(df.write)\n",
      "    <class '...readwriter.DataFrameWriter'>\n",
      "    \n",
      "    Write the DataFrame as a table.\n",
      "    \n",
      "    >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
      "    >>> df.write.saveAsTable(\"tab2\")\n",
      "    >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataframe.DataFrame.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c4e121d-91fe-4401-beb9-baa5e06e8057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| XYZ|\n",
      "|  2| ABC|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [(1,\"XYZ\"),(2,\"ABC\")]\n",
    "schema2 = [\"id\",\"name\"]\n",
    "\n",
    "df6 = spark.createDataFrame(data = data2,schema= schema2)\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d7eb2ef-da11-4160-a812-ebd0fb90cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True).csv('/home/hdoop/Docs/spark_files/writer_folder/pyspark_practice_2_10Mar2025/first_write')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646e58a-b984-4c38-9cd6-cf651b59d639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
