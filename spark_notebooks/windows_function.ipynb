{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40984ea-7de3-4514-8aa0-2cdb4fdb714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f834a3-d6ab-41cd-beea-f16a35235a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 19:58:04 WARN Utils: Your hostname, TTNPL-8203 resolves to a loopback address: 127.0.1.1; using 192.168.163.68 instead (on interface wlp0s20f3)\n",
      "25/03/20 19:58:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/20 19:58:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7c371c381d50>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"windows fuctions\")\\\n",
    "    .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b9106d-f92b-489e-8aa4-05f7b0a70dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8be7f81-fd3f-4e05-8c79-9a4669cfb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5dd65f-ea11-4c88-a67e-021c14052511",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_Spec = Window.partitionBy(\"department\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e580f1-064c-47d2-a06c-2f59bdbacc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row_number\",row_number().over(window_Spec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "448dd1de-576e-437d-8ed1-971696a678d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 20:11:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/20 20:11:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/20 20:11:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|        Maria|   Finance|  3000|         3|\n",
      "|        James|     Sales|  3000|         4|\n",
      "|         Jeff| Marketing|  3000|         5|\n",
      "|        Scott|   Finance|  3300|         6|\n",
      "|          Jen|   Finance|  3900|         7|\n",
      "|       Robert|     Sales|  4100|         8|\n",
      "|         Saif|     Sales|  4100|         9|\n",
      "|      Michael|     Sales|  4600|        10|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row_number\",row_number().over(Window.orderBy(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65333e2d-46b8-4d47-a96b-b40108b7a3f5",
   "metadata": {},
   "source": [
    "#### for rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84723a0e-72f2-41b9-b4bf-58acd5deae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5abb87-afec-4840-9e55-8ae7fb4129f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|Maria        |Finance   |3000  |1   |\n",
      "|Scott        |Finance   |3300  |2   |\n",
      "|Jen          |Finance   |3900  |3   |\n",
      "|Kumar        |Marketing |2000  |1   |\n",
      "|Jeff         |Marketing |3000  |2   |\n",
      "|James        |Sales     |3000  |1   |\n",
      "|James        |Sales     |3000  |1   |\n",
      "|Robert       |Sales     |4100  |3   |\n",
      "|Saif         |Sales     |4100  |3   |\n",
      "|Michael      |Sales     |4600  |5   |\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",rank().over(window_Spec)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44da89-a59c-4aa8-8038-34040427bdb1",
   "metadata": {},
   "source": [
    "#### dense_rank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f61cd3eb-138d-4b85-9e8c-b006e333cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3cb110b-f3ac-40b8-8b95-475d7581e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense-rank|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|Robert       |Sales     |4100  |2         |\n",
      "|Saif         |Sales     |4100  |2         |\n",
      "|Michael      |Sales     |4600  |3         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"dense-rank\",dense_rank().over(window_Spec)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d40c5-a421-45c0-8a4d-0e37e9f06dfe",
   "metadata": {},
   "source": [
    "#### percentage_rank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4058630c-10d9-46a8-ac39-93bceef3cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percent_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da479598-ba69-416b-ad46-3d334b5c9ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|Maria        |Finance   |3000  |0.0         |\n",
      "|Scott        |Finance   |3300  |0.5         |\n",
      "|Jen          |Finance   |3900  |1.0         |\n",
      "|Kumar        |Marketing |2000  |0.0         |\n",
      "|Jeff         |Marketing |3000  |1.0         |\n",
      "|James        |Sales     |3000  |0.0         |\n",
      "|James        |Sales     |3000  |0.0         |\n",
      "|Robert       |Sales     |4100  |0.5         |\n",
      "|Saif         |Sales     |4100  |0.5         |\n",
      "|Michael      |Sales     |4600  |1.0         |\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"percent_rank\",percent_rank().over(window_Spec)).show(truncate =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745200d-7693-4e17-b1a2-b6870562ff8a",
   "metadata": {},
   "source": [
    "#### ntile(-) - bucketing dividing the data into equal no of groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "934c18ae-6c60-4deb-a3de-749f217b610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25cccfbb-8d22-44f6-a192-5c60e593d4e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-------------+\n",
      "|employee_name|department|salary|bucket_number|\n",
      "+-------------+----------+------+-------------+\n",
      "|Maria        |Finance   |3000  |1            |\n",
      "|Scott        |Finance   |3300  |1            |\n",
      "|Jen          |Finance   |3900  |2            |\n",
      "|Kumar        |Marketing |2000  |1            |\n",
      "|Jeff         |Marketing |3000  |2            |\n",
      "|James        |Sales     |3000  |1            |\n",
      "|James        |Sales     |3000  |1            |\n",
      "|Robert       |Sales     |4100  |1            |\n",
      "|Saif         |Sales     |4100  |2            |\n",
      "|Michael      |Sales     |4600  |2            |\n",
      "+-------------+----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"bucket_number\",ntile(2).over(window_Spec)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2705a-bd30-42b9-a8a3-c17bf357f6d3",
   "metadata": {},
   "source": [
    "#### cume_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a2e27e-5e99-4c71-a0ae-255a141630fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import cume_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23a99633-8fde-4d37-a71d-46f6dd155782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----------------------+\n",
      "|employee_name|department|salary|cumulative distribution|\n",
      "+-------------+----------+------+-----------------------+\n",
      "|Maria        |Finance   |3000  |0.3333333333333333     |\n",
      "|Scott        |Finance   |3300  |0.6666666666666666     |\n",
      "|Jen          |Finance   |3900  |1.0                    |\n",
      "|Kumar        |Marketing |2000  |0.5                    |\n",
      "|Jeff         |Marketing |3000  |1.0                    |\n",
      "|James        |Sales     |3000  |0.4                    |\n",
      "|James        |Sales     |3000  |0.4                    |\n",
      "|Robert       |Sales     |4100  |0.8                    |\n",
      "|Saif         |Sales     |4100  |0.8                    |\n",
      "|Michael      |Sales     |4600  |1.0                    |\n",
      "+-------------+----------+------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.withColumn(\"cumulative distribution\",cume_dist().over(window_Spec)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf760bc-a34c-4cf9-97c5-202712623769",
   "metadata": {},
   "source": [
    "#### lag() lead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfba0ec6-6f23-4413-9642-7a21f4428f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag,lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce8da437-40bb-4f0a-a1e4-dc9c97dd11a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lag |\n",
      "+-------------+----------+------+----+\n",
      "|Maria        |Finance   |3000  |NULL|\n",
      "|Scott        |Finance   |3300  |3000|\n",
      "|Jen          |Finance   |3900  |3300|\n",
      "|Kumar        |Marketing |2000  |NULL|\n",
      "|Jeff         |Marketing |3000  |2000|\n",
      "|James        |Sales     |3000  |NULL|\n",
      "|James        |Sales     |3000  |3000|\n",
      "|Robert       |Sales     |4100  |3000|\n",
      "|Saif         |Sales     |4100  |4100|\n",
      "|Michael      |Sales     |4600  |4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"lag\",lag(df.salary,1).over(window_Spec)).show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26ff4a75-6a18-423a-8a40-7f262b343639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|Maria        |Finance   |3000  |3300|\n",
      "|Scott        |Finance   |3300  |3900|\n",
      "|Jen          |Finance   |3900  |NULL|\n",
      "|Kumar        |Marketing |2000  |3000|\n",
      "|Jeff         |Marketing |3000  |NULL|\n",
      "|James        |Sales     |3000  |3000|\n",
      "|James        |Sales     |3000  |4100|\n",
      "|Robert       |Sales     |4100  |4100|\n",
      "|Saif         |Sales     |4100  |4600|\n",
      "|Michael      |Sales     |4600  |NULL|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"lead\",lead(df.salary).over(window_Spec)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e359c4e-8430-4f8d-b7a8-4f6445cbd377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(window_Spec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0143d5d-c67c-44f2-88c7-a31297c84558",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be24bc5-55ed-4c27-96e8-f65f304d090a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
