{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6f33af-c454-407e-bc36-81def1ec35b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 12:18:36 WARN Utils: Your hostname, TTNPL-8203 resolves to a loopback address: 127.0.1.1; using 10.1.209.89 instead (on interface wlp0s20f3)\n",
      "25/03/10 12:18:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 12:18:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/10 12:18:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestPySpark\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfbc706-eb5f-4306-9299-80cfd787a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe from a csv file , infering schema and providing the header\n",
    "df = spark.read.csv('/home/hdoop/Docs/spark_files/file_CSV_10Mar2025_122000.csv',header = True,inferSchema =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ff2f96-1bd7-432d-9682-d612c73282f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80332de6-d63c-4c06-afd8-b5c3913db0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4397c052-b9b7-4883-b377-fb4b83763013",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd  = spark.sparkContext.textFile('/home/hdoop/Docs/spark_files/file_8Mar2025_172800.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee02608-7b01-4f5d-b6cd-2e1869ad2b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a text file.This contains some common text that can be used in common fils.',\n",
       " 'this is a simple text file thus is with txt.',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b1d1ca-61d3-4a88-8627-82e5a6418d8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moptions(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/hdoop/Docs/spark_files/file_CSV_10Mar2025_122000.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "df2 = spark.format(\"csv\").options(header=True,inferSchema=True).load('/home/hdoop/Docs/spark_files/file_CSV_10Mar2025_122000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0625ca1c-5fcc-476a-87ce-c95902d4762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19a3204-fea1-48c1-b1fc-13fbbe0ab4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\").filter(df[\"dept\"]==\"IT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a85b1597-0c0b-4396-bdfd-2a3173cc93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   dept|\n",
      "+-------+\n",
      "|     HR|\n",
      "|Finance|\n",
      "|     IT|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dept\").distinct().alias(\"distinct_dept\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ba232dd-2ea3-40da-843e-244070a39f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, salary: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"dept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed1b0908-8d2b-497d-b064-57685aa4b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8cf08a2-fe9a-4030-8bbd-fca2e840cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \t((\"James\",\"\",\"Smith\"),{\"id\": \"36636\",\"gender\": \"M\"}, [\"python\", \"java\"]), \n",
    "((\"Maria\",\"Anne\",\"Jones\"),{\"id\": \"39192\",\"gender\": \"F\"}, [\"python\", \"scala\"])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eea9f05-2a56-431e-bb76-d7c1b30c29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,MapType,ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3660feba-25a5-48f0-af63-ae696f0e6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField(\"details\", MapType(StringType(), StringType()), True),\n",
    "         StructField(\"languages\", ArrayType(StringType()), True)\n",
    "         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2069cb07-18cb-4f5d-bc39-e64abfcda35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(data = data,schema = structureSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eac4c1fc-c9a8-4c22-8d4d-8054e20d7ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+---------------+\n",
      "|name                |details                   |languages      |\n",
      "+--------------------+--------------------------+---------------+\n",
      "|{James, , Smith}    |{gender -> M, id -> 36636}|[python, java] |\n",
      "|{Maria, Anne, Jones}|{gender -> F, id -> 39192}|[python, scala]|\n",
      "+--------------------+--------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84354284-c607-4fb3-9d2b-bfef0d25696e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"name\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirstname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"name\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\"."
     ]
    }
   ],
   "source": [
    "new_df.select(df.name.firstname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "925a2b9c-c478-4691-96c7-15af0db2683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac883c5-070f-4644-8f1e-6f06bd4a1552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "|id |name    |dept   |salary|\n",
      "+---+--------+-------+------+\n",
      "|1  |mayank  |IT     |65000 |\n",
      "|2  |Sattu   |HR     |78000 |\n",
      "|3  |Akrash  |Finance|66000 |\n",
      "|4  |Himanshu|IT     |85000 |\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "085f7254-e848-4f38-806d-68b982ae6ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+\n",
      "| id|    name|   dept|salary|\n",
      "+---+--------+-------+------+\n",
      "|  1|  mayank|     IT| 65000|\n",
      "|  2|   Sattu|     HR| 78000|\n",
      "|  3|  Akrash|Finance| 66000|\n",
      "|  4|Himanshu|     IT| 85000|\n",
      "+---+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "722c733a-3988-46d0-8cb8-a68fef422076",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toColumn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoColumn\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_column_with a_value_to_truncate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toColumn'"
     ]
    }
   ],
   "source": [
    "df.toColumn(\"new_column_with a_value_to_truncate\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ec6f00c-0f98-475e-a8b1-d759cdb7acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  mayank|\n",
      "|   Sattu|\n",
      "|  Akrash|\n",
      "|Himanshu|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1a2950f-fbeb-4bf1-a320-1adbb81de9d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"name\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmiddlename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"name\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\"."
     ]
    }
   ],
   "source": [
    "df.select(df.name.middlename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5639e83f-6215-4763-a3dc-81b17df248b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'details'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_df\u001b[38;5;241m.\u001b[39mselect(df\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mfirstname, df\u001b[38;5;241m.\u001b[39mname[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlastname\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetails\u001b[49m\u001b[38;5;241m.\u001b[39mgender, df\u001b[38;5;241m.\u001b[39mdetails[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], df\u001b[38;5;241m.\u001b[39mlanguages[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'details'"
     ]
    }
   ],
   "source": [
    "new_df.select(df.name.firstname, df.name[\"lastname\"], df.details.gender, df.details[\"id\"], df.languages[0]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f41b06d-b0f6-4e05-bf2d-9e79eb921095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, MapType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25557d21-2258-4423-9b0c-41a322504cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    \t((\"James\",\"\",\"Smith\"),{\"id\": \"36636\",\"gender\": \"M\"}, [\"python\", \"java\"]), \n",
    "((\"Maria\",\"Anne\",\"Jones\"),{\"id\": \"39192\",\"gender\": \"F\"}, [\"python\", \"scala\"])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ade1f670-f9ca-4c2a-867c-0c3665708afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField(\"details\", MapType(StringType(), StringType()), True),\n",
    "         StructField(\"languages\", ArrayType(StringType()), True)\n",
    "         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3bd07e7-c2bd-413b-8171-e2d2a31c080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=structureSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2905370a-daeb-47f8-90fd-b53c5daa3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- details: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e06e550-13fd-44fc-8a83-f091ce2d89c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+---------------+\n",
      "|name                |details                   |languages      |\n",
      "+--------------------+--------------------------+---------------+\n",
      "|{James, , Smith}    |{gender -> M, id -> 36636}|[python, java] |\n",
      "|{Maria, Anne, Jones}|{gender -> F, id -> 39192}|[python, scala]|\n",
      "+--------------------+--------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "007bf410-a5cf-4abc-a5f2-ce2fc331978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------+-----------+------------+\n",
      "|name.firstname|name.lastname|details[gender]|details[id]|languages[0]|\n",
      "+--------------+-------------+---------------+-----------+------------+\n",
      "|         James|        Smith|              M|      36636|      python|\n",
      "|         Maria|        Jones|              F|      39192|      python|\n",
      "+--------------+-------------+---------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name.firstname, df.name[\"lastname\"], df.details.gender, df.details[\"id\"], df.languages[0]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f5c7ca4-6e0a-459c-950d-2eb3ed6e9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- details: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9687d837-6423-423f-9748-a97498317f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------+-----------+------------+\n",
      "|name.firstname|name.lastname|details[gender]|details[id]|languages[0]|\n",
      "+--------------+-------------+---------------+-----------+------------+\n",
      "|         James|        Smith|              M|      36636|      python|\n",
      "|         Maria|        Jones|              F|      39192|      python|\n",
      "+--------------+-------------+---------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(new_df.name.firstname, new_df.name[\"lastname\"], new_df.details.gender, new_df.details[\"id\"], new_df.languages[0]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f678ee-ef85-4d76-95d8-ec0945da4f92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b162c8-f06a-40c2-ba37-660752a7ae64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
