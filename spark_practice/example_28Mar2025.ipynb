{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9768b453-e133-4ebf-9f36-85a63564ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adcb9f94-849b-4bc0-a306-861591710697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark  import broadcast\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14e4e75d-3c34-48a7-add9-c3525bffc811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7bfab1781c60>\n"
     ]
    }
   ],
   "source": [
    "spark =  SparkSession.builder.appName(\"example_28Mar2025\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117acc0-0864-4fab-8fd3-ed7a6e47e17b",
   "metadata": {},
   "source": [
    "## Problem Statement 1\n",
    "You are given a dataset containing customer travel details, including multiple stopovers for flight journeys. The objective is to determine the origin (starting location) and destination (final location) for each customer based on their travel history.\n",
    "\n",
    "| customer_id | flight_id | origin   | destination |\n",
    "|------------|----------|---------|-------------|\n",
    "| 1          | 101      | Delhi   | Hyderabad   |\n",
    "| 1          | 102      | Hyderabad | Kochi      |\n",
    "| 1          | 103      | Kochi   | Mangalore   |\n",
    "| 2          | 201      | Mumbai  | Ayodhya     |\n",
    "| 2          | 202      | Ayodhya | Gorakhpur   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdd71772-28c9-4ebb-9e10-db25891beec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"True\")\\\n",
    "    .option(\"inferSchema\",\"True\")\\\n",
    "    .load(\"file:///home/hdoop/notebooks/data/spark_practice/example_28Mar2025/flights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df3c3c04-fa98-4139-b387-a44ec66db657",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"flight_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a7a090d-62bf-46ce-b9b3-9905548b5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+-----------+\n",
      "|customer_id|flight_id|   origin|destination|\n",
      "+-----------+---------+---------+-----------+\n",
      "|          1|      101|    Delhi|  Hyderabad|\n",
      "|          1|      102|Hyderabad|      Kochi|\n",
      "|          1|      103|    Kochi|  Mangalore|\n",
      "|          2|      201|   Mumbai|    Ayodhya|\n",
      "|          2|      202|  Ayodhya|  Gorakhpur|\n",
      "+-----------+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7d72075-9381-4acb-8a9f-c8ea68c6b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =df.withColumn(\"origin\",first(\"origin\").over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f77728d8-c131-4dbe-9992-a950d572ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =  df1.withColumn(\"destination\",first(\"destination\").over(Window.partitionBy(\"customer_id\").orderBy(df[\"flight_id\"].desc()))).drop(\"flight_id\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ce41627-7706-4f6c-a98c-66b2590ffdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------+\n",
      "|customer_id|origin|destination|\n",
      "+-----------+------+-----------+\n",
      "|          1| Delhi|  Mangalore|\n",
      "|          2|Mumbai|  Gorakhpur|\n",
      "+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0dcb4-27ba-4b30-b62a-7b27f870e8df",
   "metadata": {},
   "source": [
    "# Employee Dataset Merging Task\n",
    "\n",
    "## 📌 Task Description\n",
    "You are given **two employee datasets** containing details about employees, including their **names, gender, and salaries**. Your task is to **combine these datasets** using:\n",
    "\n",
    "1. **`union()`** - Merging datasets **without removing duplicates**.\n",
    "2. **Handling duplicate records** using **`distinct()`**.\n",
    "3. **`unionByName()`** - Merging datasets when column names match but the order differs.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Employee Data\n",
    "\n",
    "### **Dataset 1 (`df1`)**\n",
    "| Employee_Name | Employee_Gender | Employee_Salary |\n",
    "|--------------|----------------|----------------|\n",
    "| Alice        | F              | 50000          |\n",
    "| Bob          | M              | 60000          |\n",
    "| Charles      | M              | 70000          |\n",
    "| David        | M              | 80000          |\n",
    "| Eve          | F              | 90000          |\n",
    "| Eve          | F              | 90000          | *(Duplicate for testing `distinct()`)* |\n",
    "\n",
    "### **Dataset 2 (`df2`)**\n",
    "| Employee_Name | Employee_Gender | Employee_Salary |\n",
    "|--------------|----------------|----------------|\n",
    "| Frank        | M              | 55000          |\n",
    "| Grace        | F              | 65000          |\n",
    "| Hank         | M              | 75000          |\n",
    "| Ivy          | F              | 85000          |\n",
    "| Jack         | M              | 95000          |\n",
    "| Eve          | F              | 90000          | *(Duplicate for testing `distinct()`)* |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Your Tasks**\n",
    "1. **Use `union()`** to merge `df1` and `df2`, keeping duplicates.\n",
    "2. **Use `distinct()`** to remove duplicate records.\n",
    "3. **Use `unionByName()`** to merge the datasets while handling **different column orders**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb87400c-f4bb-450f-a7ca-d2a1cf4c94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\").load(\"file:///home/hdoop/notebooks/data/spark_practice/example_28Mar2025/employee_merging_task/emp1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15499cb2-935e-4aa2-8ef1-de2c17e282b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\").load(\"file:///home/hdoop/notebooks/data/spark_practice/example_28Mar2025/employee_merging_task/emp2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48adcc0b-e654-4641-af63-293ac123ff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n",
      "|Employee_Name|Employee_Gender|Employee_Salary|\n",
      "+-------------+---------------+---------------+\n",
      "|        Alice|              F|          50000|\n",
      "|          Bob|              M|          60000|\n",
      "|      Charles|              M|          70000|\n",
      "|        David|              M|          80000|\n",
      "|          Eve|              F|          90000|\n",
      "|          Eve|              F|          90000|\n",
      "+-------------+---------------+---------------+\n",
      "\n",
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Employee_Gender: string (nullable = true)\n",
      " |-- Employee_Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "897ca6af-f457-45f6-b7de-e49d85049167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n",
      "|Employee_Name|Employee_Gender|Employee_Salary|\n",
      "+-------------+---------------+---------------+\n",
      "|        Frank|              M|          55000|\n",
      "|        Grace|              F|          65000|\n",
      "|         Hank|              M|          75000|\n",
      "|          Ivy|              F|          85000|\n",
      "|         Jack|              M|          95000|\n",
      "|          Eve|              F|          90000|\n",
      "+-------------+---------------+---------------+\n",
      "\n",
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Employee_Gender: string (nullable = true)\n",
      " |-- Employee_Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6927a5b4-7409-48b1-beb5-262e534a1af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n",
      "|Employee_Name|Employee_Gender|Employee_Salary|\n",
      "+-------------+---------------+---------------+\n",
      "|        Alice|              F|          50000|\n",
      "|          Bob|              M|          60000|\n",
      "|      Charles|              M|          70000|\n",
      "|        David|              M|          80000|\n",
      "|          Eve|              F|          90000|\n",
      "|          Eve|              F|          90000|\n",
      "|        Frank|              M|          55000|\n",
      "|        Grace|              F|          65000|\n",
      "|         Hank|              M|          75000|\n",
      "|          Ivy|              F|          85000|\n",
      "|         Jack|              M|          95000|\n",
      "|          Eve|              F|          90000|\n",
      "+-------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a62b798f-c558-4dda-9462-2d7210fd74a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n",
      "|Employee_Name|Employee_Gender|Employee_Salary|\n",
      "+-------------+---------------+---------------+\n",
      "|        Alice|              F|          50000|\n",
      "|          Bob|              M|          60000|\n",
      "|      Charles|              M|          70000|\n",
      "|        David|              M|          80000|\n",
      "|          Eve|              F|          90000|\n",
      "|          Eve|              F|          90000|\n",
      "|        Frank|              M|          55000|\n",
      "|        Grace|              F|          65000|\n",
      "|         Hank|              M|          75000|\n",
      "|          Ivy|              F|          85000|\n",
      "|         Jack|              M|          95000|\n",
      "|          Eve|              F|          90000|\n",
      "+-------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionByName(df2,allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c4d9c59-18e7-448f-a957-5d300de6a385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n",
      "|Employee_Name|Employee_Gender|Employee_Salary|\n",
      "+-------------+---------------+---------------+\n",
      "|        David|              M|          80000|\n",
      "|      Charles|              M|          70000|\n",
      "|        Alice|              F|          50000|\n",
      "|          Eve|              F|          90000|\n",
      "|          Bob|              M|          60000|\n",
      "|        Grace|              F|          65000|\n",
      "|         Hank|              M|          75000|\n",
      "|         Jack|              M|          95000|\n",
      "|        Frank|              M|          55000|\n",
      "|          Ivy|              F|          85000|\n",
      "+-------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionAll(df2).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a776206-ac2c-4181-ae55-6aa17e8cd363",
   "metadata": {},
   "source": [
    "# 📌 Problem Statement 2\n",
    "\n",
    "## 📝 Task\n",
    "You are given an **Employee table** containing employee details such as:\n",
    "- **Employee_ID**\n",
    "- **Employee_Name**\n",
    "- **Department**\n",
    "- **Salary**\n",
    "\n",
    "Your task is to **write a PySpark program** to compare the behavior of the following **three ranking functions**:  \n",
    "\n",
    "1. **`rank()`** - Assigns a unique rank, leaving gaps if there are duplicates.  \n",
    "2. **`dense_rank()`** - Similar to `rank()`, but does not leave gaps when encountering duplicate values.  \n",
    "3. **`row_number()`** - Assigns a unique sequential number to each row, even for duplicate values.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Employee Dataset (Example)**\n",
    "\n",
    "| Employee_ID | Employee_Name | Department | Salary |\n",
    "|------------|--------------|------------|--------|\n",
    "| 1          | Alice        | IT         | 70000  |\n",
    "| 2          | Bob          | IT         | 75000  |\n",
    "| 3          | Charlie      | HR         | 60000  |\n",
    "| 4          | David        | IT         | 75000  |\n",
    "| 5          | Eve          | HR         | 60000  |\n",
    "| 6          | Frank        | Finance    | 90000  |\n",
    "| 7          | Grace        | Finance    | 85000  |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Your Tasks**\n",
    "1. **Use `rank()` to assign ranks based on salary** (ordered in descending order).  \n",
    "2. **Use `dense_rank()` and compare how ranks are assigned differently from `rank()`**.  \n",
    "3. **Use `row_number()` to understand how it uniquely assigns numbers to each row**.  \n",
    "4. **Compare the results of all three ranking functions in PySpark**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Expected Output Example\n",
    "\n",
    "| Employee_Name | Department | Salary | Rank | Dense_Rank | Row_Number |\n",
    "|--------------|------------|--------|------|------------|------------|\n",
    "| Frank        | Finance    | 90000  | 1    | 1          | 1          |\n",
    "| Grace        | Finance    | 85000  | 2    | 2          | 2          |\n",
    "| Bob          | IT         | 75000  | 3    | 3          | 3          |\n",
    "| David        | IT         | 3      | 75000 | 3          | 4          |\n",
    "| Alice        | IT         | 70000  | 5    | 4          | 5          |\n",
    "| Charlie      | HR         | 60000  | 6    | 5          | 6          |\n",
    "| Eve          | HR         | 60000  | 6    | 5          | 7          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9e43a56-f37e-4cc9-a091-69a68a72c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\").load(\"file:///home/hdoop/notebooks/data/spark_practice/example_28Mar2025/emp_ps2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e637a1e-5a31-4b68-b1a6-e54dd83dbf87",
   "metadata": {},
   "source": [
    "#### Rank(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a966ed1-9ccb-4870-9f68-d419e4453a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_ID: integer (nullable = true)\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n",
      "+-----------+-------------+----------+------+\n",
      "|Employee_ID|Employee_Name|Department|Salary|\n",
      "+-----------+-------------+----------+------+\n",
      "|          1|        Alice|     Sales| 75000|\n",
      "|          2|          Bob|     Sales| 75000|\n",
      "|          3|      Charlie|     Sales| 75000|\n",
      "|          4|        David|        IT| 70000|\n",
      "|          5|          Eva|        IT| 68000|\n",
      "|          6|        Frank|        IT| 68000|\n",
      "|          7|        Grace|        HR| 60000|\n",
      "|          8|        Henry|        HR| 60000|\n",
      "|          9|          Ian|        HR| 58000|\n",
      "|         10|         Jack|        HR| 56000|\n",
      "+-----------+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9133391b-1fcc-4790-937f-78f0ba55d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+------+----+\n",
      "|Employee_ID|Employee_Name|Department|Salary|rank|\n",
      "+-----------+-------------+----------+------+----+\n",
      "|         10|         Jack|        HR| 56000|   1|\n",
      "|          9|          Ian|        HR| 58000|   2|\n",
      "|          7|        Grace|        HR| 60000|   3|\n",
      "|          8|        Henry|        HR| 60000|   3|\n",
      "|          5|          Eva|        IT| 68000|   1|\n",
      "|          6|        Frank|        IT| 68000|   1|\n",
      "|          4|        David|        IT| 70000|   3|\n",
      "|          1|        Alice|     Sales| 75000|   1|\n",
      "|          2|          Bob|     Sales| 75000|   1|\n",
      "|          3|      Charlie|     Sales| 75000|   1|\n",
      "+-----------+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",rank().over(Window.partitionBy(\"Department\").orderBy(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30b796-589f-4539-8325-8adb0ad846d9",
   "metadata": {},
   "source": [
    "#### dense Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f69adaf9-2b5a-489d-981b-c626d7f8db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+------+----------+\n",
      "|Employee_ID|Employee_Name|Department|Salary|dense_rank|\n",
      "+-----------+-------------+----------+------+----------+\n",
      "|         10|         Jack|        HR| 56000|         1|\n",
      "|          9|          Ian|        HR| 58000|         2|\n",
      "|          7|        Grace|        HR| 60000|         3|\n",
      "|          8|        Henry|        HR| 60000|         3|\n",
      "|          5|          Eva|        IT| 68000|         1|\n",
      "|          6|        Frank|        IT| 68000|         1|\n",
      "|          4|        David|        IT| 70000|         2|\n",
      "|          1|        Alice|     Sales| 75000|         1|\n",
      "|          2|          Bob|     Sales| 75000|         1|\n",
      "|          3|      Charlie|     Sales| 75000|         1|\n",
      "+-----------+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"dense_rank\",dense_rank().over(Window.partitionBy(\"department\").orderBy(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644b69d-4610-4a16-9e01-1f5574fc6c51",
   "metadata": {},
   "source": [
    "# 🏏 Cricket Match Statistics - Pivoting Data in PySpark\n",
    "\n",
    "## 📌 Problem Statement\n",
    "\n",
    "You are given a **cricket match dataset** containing the following columns:\n",
    "\n",
    "- **player_name** → Name of the player.\n",
    "- **stadium** → The stadium where the match was played.\n",
    "- **runs** → Runs scored by the player.\n",
    "- **wickets** → Wickets taken by the player.\n",
    "- **catches** → Catches taken by the player.\n",
    "\n",
    "### 🎯 **Task**\n",
    "- Keep **player_name** as a row index.\n",
    "- Convert **stadium** values into columns.\n",
    "- Compute the **sum of runs, wickets, and catches** for each stadium.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Cricket Dataset**\n",
    "\n",
    "| player_name | stadium       | runs | wickets | catches |\n",
    "|------------|--------------|------|---------|---------|\n",
    "| JJA        | Wankhede     | 40   | 2       | 1       |\n",
    "| JJA        | Wankhede     | 60   | 1       | 2       |\n",
    "| JJA        | Eden Gardens | 25   | 3       | 0       |\n",
    "| Hardik     | Eden Gardens | 55   | 2       | 1       |\n",
    "| Hardik     | Eden Gardens | 30   | 1       | 2       |\n",
    "| Hardik     | Eden Gardens | 45   | 2       | 1       |\n",
    "| Hardik     | Wankhede     | 45   | 3       | 2       |\n",
    "| Watson     | Eden Gardens | 20   | 1       | 3       |\n",
    "| Watson     | Wankhede     | 50   | 4       | 2       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ceb37b-2306-4b06-ad20-d3e0747f6d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
