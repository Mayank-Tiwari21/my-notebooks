{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf454ed-50f4-444f-90e6-882101dcc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb69e44b-40dc-4088-b17a-48f1e6cbf486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 13:49:57 WARN Utils: Your hostname, TTNPL-8203 resolves to a loopback address: 127.0.1.1; using 10.1.209.120 instead (on interface wlp0s20f3)\n",
      "25/03/24 13:49:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/24 13:49:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7b8e0b52f4f0>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"find the genius\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71aa47b3-6611-4fea-8f67-68a739f7038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (100, \"HR\", 5000, \"2023-01-01\"),\n",
    "    (100, \"Finance\", 6000, \"2023-02-01\"),\n",
    "    (200, \"IT\", 7000, \"2023-03-01\"),\n",
    "    (300, \"Sales\", 5500, \"2023-04-01\"),\n",
    "    (300, \"Marketing\", 6500, \"2023-05-01\"),\n",
    "    (300, \"HR\", 7500, \"2023-06-01\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data = data,schema = [\"id\",\"department\",\"salary\",\"date\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d974139-c02c-4eda-9bbd-9c9ed1b03c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df2 = df.groupBy(\"id\").agg(\n",
    "    count(\"id\").alias(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664ff097-ab7a-47db-98b7-eeedea90f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------+\n",
      "| id|department|salary|      date|\n",
      "+---+----------+------+----------+\n",
      "|200|        IT|  7000|2023-03-01|\n",
      "+---+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2.where(df2[\"count\"]==1),on=\"id\").drop(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b102a042-8a39-4261-a023-455e49b6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------+\n",
      "| id|department|salary|      date|\n",
      "+---+----------+------+----------+\n",
      "|200|        IT|  7000|2023-03-01|\n",
      "+---+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2.where(df2[\"count\"] == 1),\"id\",how = \"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cbb9116-ed11-4788-9d7e-cce9ecfec93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"1\",\"2\",\"4\",\"5\",\"4\",\"acb\",\"nsjn\",\"bjbs\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd2  =rdd.map(lambda x:(x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5619c784-5406-48ce-b31b-c99b6e8e6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd2.toDF([\"cols\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7093cee8-82b5-4c61-85ef-e64937cac57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544d0819-e28b-43d5-9896-23e50e82f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\"cols\",df.cols.cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "105b0dca-a960-44ea-bdcd-84a8da350dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|cols|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|   4|\n",
      "|   5|\n",
      "|   4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.cols.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1dd0cc5f-6944-43be-8703-9db4a0d77108",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, None, 10), (2, None, None), (None, 3, 30)]\n",
    "columns = [\"column_one\", \"column_two\", \"column_three\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad0a3d2b-4d5e-4d8f-91e9-9ea33412ffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column_one: long (nullable = true)\n",
      " |-- column_two: long (nullable = true)\n",
      " |-- column_three: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbb67b46-9612-4e0b-9599-0ccdba4406d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict ={}\n",
    "for column in df.columns:\n",
    "    dict[column]=df.filter(df[column].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d22bcef-9fa3-40ce-b7a7-8f08ff6c393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls = spark.createDataFrame(data = tuple(dict.items()),schema = [\"column\",\"null_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8d3bffb-53e5-4f0e-90ef-c7a9f761979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|      column|null_count|\n",
      "+------------+----------+\n",
      "|  column_one|         1|\n",
      "|  column_two|         2|\n",
      "|column_three|         1|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6999573f-d20d-4f66-9fff-3842503f3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"John\", \"HR\", 5000),\n",
    "    (\"Smith\", \"Finance\", 6000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f29ba86-a96c-4f57-b78a-73dc2e5ff42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data = data , schema = [\"name\",\"dept\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ed9ec6f-d969-4985-9556-1bc0866b4506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "de_name\n",
      "dept\n",
      "de_dept\n",
      "salary\n",
      "de_salary\n"
     ]
    }
   ],
   "source": [
    "prefix =\"de_\"\n",
    "col_list = []\n",
    "for columns in df.columns:\n",
    "    print(columns)\n",
    "    new_columns = prefix+columns\n",
    "    print(new_columns)\n",
    "    df = df.withColumnRenamed(existing=columns,new=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2069c606-f573-4765-b548-d33c012963b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|de_name|de_dept|de_salary|\n",
      "+-------+-------+---------+\n",
      "|   John|     HR|     5000|\n",
      "|  Smith|Finance|     6000|\n",
      "+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104d287-1f43-4d89-b766-e22af71fc221",
   "metadata": {},
   "source": [
    "#### Flattening an array column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff16d85e-1c75-4091-ad66-138b40b15e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, [\"mobile\", \"PC\", \"tab\"]),\n",
    "    (2, [\"mobile\", \"PC\"]),\n",
    "    (3, [\"tab\", \"pen\"]),\n",
    "]\n",
    "df = spark.createDataFrame(data = data,schema = [\"cusomer_id\",\"product_purchased\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e1e65dc-94cf-41ec-9da7-916c3ed66955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cusomer_id: long (nullable = true)\n",
      " |-- product_purchased: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "254343f3-0942-4994-8c9c-fa3bc007e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\"product_Purchased\",f.explode(df.product_purchased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48eb6138-24fd-4a81-be86-0dbf3d1423cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|cusomer_id|product_purchased|\n",
      "+----------+-----------------+\n",
      "|         1|[mobile, PC, tab]|\n",
      "|         2|     [mobile, PC]|\n",
      "|         3|       [tab, pen]|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29ea010c-85dc-442d-9158-d9ec7d509a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|cusomer_id|product_Purchased|\n",
      "+----------+-----------------+\n",
      "|         1|           mobile|\n",
      "|         1|               PC|\n",
      "|         1|              tab|\n",
      "|         2|           mobile|\n",
      "|         2|               PC|\n",
      "|         3|              tab|\n",
      "|         3|              pen|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a6bbf-f9a1-4e5a-8c8d-3975506e54ae",
   "metadata": {},
   "source": [
    "#### cumulative sales in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a35c8981-914c-4da0-ad24-e096688e1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "476279c6-c361-45b5-aeb4-1e48bf02e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- sales_date: date (nullable = true)\n",
      " |-- sales_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [(1, '2024-03-01', 100), (1, '2024-03-02', 200),\n",
    "              (2, '2024-03-01', 150), (2, '2024-03-03', 300)]\n",
    "df = spark.createDataFrame(data =sales_data , schema = [\"product_id\", \"sales_date\", \"sales_amount\"])\n",
    "df =df.withColumn(\"sales_date\",df.sales_date.cast(\"Date\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c5e90-c347-452a-9cce-805004ac224f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "48c4e68b-9841-45f5-b8a2-b4f290869203",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"sales_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b5989d82-2e4e-4fee-8b12-f509f6585bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+--------------+\n",
      "|product_id|sales_date|sales_amount|cumulative_sum|\n",
      "+----------+----------+------------+--------------+\n",
      "|         1|2024-03-01|         100|           100|\n",
      "|         1|2024-03-02|         200|           300|\n",
      "|         2|2024-03-01|         150|           150|\n",
      "|         2|2024-03-03|         300|           450|\n",
      "+----------+----------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"cumulative_sum\",f.sum(df[\"sales_amount\"]).over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62913a7-03ae-4fa7-9633-cc3d77ee2dcf",
   "metadata": {},
   "source": [
    "#### previous sales and next sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1ce34e7e-39e8-4d58-94d3-b030bb972b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+--------------+----------+\n",
      "|product_id|sales_date|sales_amount|previous_sales|next_sales|\n",
      "+----------+----------+------------+--------------+----------+\n",
      "|         1|2024-03-01|         100|          NULL|       200|\n",
      "|         1|2024-03-02|         200|           100|      NULL|\n",
      "|         2|2024-03-01|         150|          NULL|       300|\n",
      "|         2|2024-03-03|         300|           150|      NULL|\n",
      "+----------+----------+------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"previous_sales\",f.lag(\"sales_amount\").over(window_spec))\\\n",
    "    .withColumn(\"next_sales\",f.lead(\"sales_amount\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "172b5602-c6a3-46fa-92e7-6fceac456156",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = df.agg(f.max(\"sales_date\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "56dfe86c-36ee-400d-83f9-af40a7585300",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = df.agg(f.min(\"sales_date\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "916eebca-bfff-427b-aa02-1c3d3193779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-03 2024-03-01\n"
     ]
    }
   ],
   "source": [
    "print(max_date,min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1267f983-dbcf-4b02-a05b-29de5c5032a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta,datetime,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "01310876-de74-4401-b258-86b5224df794",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = min_date\n",
    "end_date = max_date\n",
    "current_date = start_date\n",
    "date_list = []\n",
    "while current_date<=end_date:\n",
    "    date_list.append(current_date)\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "71ed90dd-408a-46a4-8ef2-2baaf86ab80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2024, 3, 1), datetime.date(2024, 3, 2), datetime.date(2024, 3, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f63d35d8-715f-4c42-95d2-21c2185831bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(date_list)\n",
    "rdd2 =rdd.map(lambda x:(x,))\n",
    "df1 = rdd2.toDF([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c0d18462-08ca-41dc-9cb1-a9a8f8b8e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- sales_date: date (nullable = true)\n",
      " |-- sales_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3da178db-d067-4e99-b04b-dc06509265d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "682bf7a7-2311-44a3-9c23-4af7df2dfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2024-03-01|\n",
      "|2024-03-02|\n",
      "|2024-03-03|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a0944d68-a968-4ba5-a2a5-2dda95b11d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|date|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df,df[\"sales_date\"]==df1[\"date\"],\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8cd2482d-9beb-47a6-9e07-d59f6bdcab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 16:46:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/24 16:46:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/24 16:46:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+----+\n",
      "|product_id|sales_date|sales_amount|rank|\n",
      "+----------+----------+------------+----+\n",
      "|         2|2024-03-03|         300|   1|\n",
      "|         1|2024-03-02|         200|   2|\n",
      "|         2|2024-03-01|         150|   3|\n",
      "|         1|2024-03-01|         100|   4|\n",
      "+----------+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",f.rank().over(Window.orderBy(f.col(\"sales_amount\").desc()))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "abb92ea9-7f11-42ad-916e-7f27a56e9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "78be4d3e-55db-4044-8e4b-13ea99344b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = Row(product_id = 3,sales_date = date(2024,3,5),sales_amount=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "89492b39-b42f-47a8-b6e0-764a826f0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_rw = spark.createDataFrame([new_row],[\"product_id\", \"sales_date\", \"sales_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "38f5b05f-0539-40b0-964b-60bf9b5bb389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.union(new_df_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1b66cb16-62c2-46df-8d80-3f208c56599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|product_id|sales_date|sales_amount|\n",
      "+----------+----------+------------+\n",
      "|         1|2024-03-01|         100|\n",
      "|         1|2024-03-02|         200|\n",
      "|         2|2024-03-01|         150|\n",
      "|         2|2024-03-03|         300|\n",
      "|         3|2024-03-05|         500|\n",
      "+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ef212c89-3912-46b9-8151-5d053f767403",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b166082-4cb3-41cf-afb7-6da108a50df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
