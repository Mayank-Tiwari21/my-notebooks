spark streaming done
spark done
airflow
word count
screen shot code done
writing mode in spark streaming done

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, count, desc
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("StreamingTransactions") \
    .config("spark.sql.streaming.checkpointLocation", "/home/hadoopuser/Downloads/checkpoint") \
    .getOrCreate()

# Define Schema for Transactions
schema = StructType([
    StructField("transaction_id", IntegerType(), True),
    StructField("customer_id", IntegerType(), True),
    StructField("product_id", IntegerType(), True),
    StructField("amount", DoubleType(), True),
    StructField("timestamp", StringType(), True)
])

# Read Transactions Dataset as a Streaming DataFrame
input_path = "/home/hadoopuser/Downloads/transactions_streaming"
transactions_df = spark.readStream \
    .schema(schema) \
    .json(input_path)

# Display New Transactions in Real-Time
query1 = transactions_df.writeStream \
    .format("console") \
    .outputMode("append") \
    .start()

# Write Streaming Transactions to CSV with Checkpointing
output_path = "/home/hadoopuser/Downloads/stream_output"
query2 = transactions_df.writeStream \
    .format("csv") \
    .option("path", output_path) \
    .option("checkpointLocation", "/home/hadoopuser/Downloads/checkpoint") \
    .outputMode("append") \
    .start()

# Compute Real-Time Total Sales Per Product
total_sales_df = transactions_df.groupBy("product_id").agg(
    sum("amount").alias("total_sales")
)

query3 = total_sales_df.writeStream \
    .format("console") \
    .outputMode("complete") \
    .start()

# Find Real-Time Top 5 Best-Selling Products
top_products_df = total_sales_df.orderBy(desc("total_sales")).limit(5)

query4 = top_products_df.writeStream \
    .format("console") \
    .outputMode("complete") \
    .start()

# Write Streaming Output to PostgreSQL Table
postgresql_url = "jdbc:postgresql://localhost:5432/yourdatabase"
postgresql_properties = {
    "user": "yourusername",
    "password": "yourpassword",
    "driver": "org.postgresql.Driver"
}

def write_to_postgres(batch_df, batch_id):
    batch_df.write \
        .jdbc(url=url, table="orders", mode="append", properties=propert)

query5 = top_products_df.writeStream \
    .foreachBatch(write_to_postgres) \
    .outputMode("complete") \
    .start()

# Wait for Streaming Queries to Finish
query1.awaitTermination()
query2.awaitTermination()
query3.awaitTermination()
query4.awaitTermination()
query5.awaitTermination()

